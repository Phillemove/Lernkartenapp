Sigmoid Function	"<img class=latex src=""latex-fc961abcb39c04e8cd25d237e87ea6f4055db8bd.png""><div>Always crosses y axis at 0.5</div>"
Softplus	smoothed out ReLU&nbsp;function
Bayes Rule	"<img class=latex src=""latex-16c29011c5fed8f2ad9bedfdff3a83222278b509.png"">"
Self information of an event	"<img class=latex src=""latex-ad5acac3fb84626d605ba4c60bcfce40207bc146.png"">"
L1 Regularization	"<img class=latex src=""latex-cb07d4c9b4404734a0e43fc3bd02f466c12b7de9.png""><div>- favors sparse weight vectors, thereby discarding noise</div><div>- resulting in built in feature selection</div>"
L2 Regularization	"<img class=latex src=""latex-f362d338f18fcd2e043a77aa633d1571f730106a.png""><div>- Penalizes peaky weight vectors</div>"
Input for a Synapse (biological)	Dendritic tree
Components of a neuron (biological)	Dendritic tree<br>Synapse<br>Axon
Function of Synapse (biological)	Connection of dendritic tree and axon:&nbsp;<div>- synapse generates chemical signal from electric spikes</div><div>- axon generates new electrical spikes when cell membrane is depolarized (forming a threshold)</div>
Function of Axon (biological)	Connects to other dendritic trees, generates elctrical spikes when threshold is reached
ReLU	max(0, x)
Two types of supervised learning	Regression &amp; Classification
What does learning usually mean?	Adjusting parameters to reduce the difference between actual and expected output
Perceptron in the strict sense	Binary classifier (discrete output)
"<img class=latex src=""latex-03a45952f8115322d2879f7d090126f059757ba0.png"">"	Eta
Delta Rule in Backpropagation	"<img class=latex src=""latex-8722e35ef7c05c946bde5408d053c5fe98044297.png""><div><img class=latex src=""latex-03a45952f8115322d2879f7d090126f059757ba0.png""> learning rate</div><div><img class=latex src=""latex-60c4be24f70d72a397c2d2c34c53445d8831a761.png""> error<br></div><div><img class=latex src=""latex-2bda1dc998f554b2fec6cabde67062729f16b224.png""> derivation of activation function<br></div><div><img class=latex src=""latex-247e5951757ccac96f3a138f5ad6a852e23c5bf4.png""> for a linear activation function<br></div><div><img class=latex src=""latex-67bc6daa9d6b964201d6cef60cbeb1ac5fd26ead.png""> input<br></div>"
Sigmoid Function Derivation	"<img class=latex src=""latex-311dac8436b3bf5b36a73fa29e80e9522947dc7d.png"">"
Backpropagation: Error Term for Output Layer - what is its formula, what is it used for?	"<img class=latex src=""latex-59efccf6e80a5cea9d056c6cd8dcdfb42ea4fbeb.png""><div>Used in the update rule for output layer's weights</div>"
Backpropagation: Error Term for Hidden Layer (with sigmoid activation function)	"<img class=latex src=""latex-db60c174790496d681d04dcdf3d56c6912d9847f.png""><div><br>Where&nbsp;</div><div><img class=latex src=""latex-d6b77a4c3f1706420d56bf3bbd19439ea86e592b.png""> is the error term of the respective output neuron</div><div><img class=latex src=""latex-0785f19824c8b23ab3dfe000b471994078fdf589.png""> is the activation value of the neuron</div><div><img class=latex src=""latex-797680ab36da0bc5728d17419ef6455be64d6428.png""> is the backpropagated error term for output <img class=latex src=""latex-8c325612684d41304b9751c175df7bcc0f61f64f.png""> of this neuron (<img class=latex src=""latex-8122aa89ea6e80784c6513d22787ad86e36ad0cc.png"">)<br></div>"
Backpropagation: Update Rule for Output Weights	"<img class=latex src=""latex-b0bde26a4644c90bc5eb00192d5634f099e8431e.png"">"
Backpropagation: Weight update rule for hidden layer's neurons	"<img class=latex src=""latex-deaa113d21588762c0748aefaa8178eeacafbe48.png""><div>where:<br><div><img class=latex src=""latex-03a45952f8115322d2879f7d090126f059757ba0.png""> learning rate</div></div><div><img class=latex src=""latex-87a697f44c6d2baa7f2a8680b06ca939b1d01ece.png""> error term for hidden layer neurons<br></div><div><img class=latex src=""latex-67bc6daa9d6b964201d6cef60cbeb1ac5fd26ead.png""> input<br></div>"
Derivation of MSE	"Recall the original formula: <img class=latex src=""latex-b0ee33c3ba25531a1f5c5056c1225a7e82323319.png""><div><br></div><div>then <img class=latex src=""latex-87042aaf121627991fc34a30e6a6259a8635ea71.png""></div>"
MLP update rule with momentum	"<img class=latex src=""latex-8b2f0c2872b1d9f3173a1869df6fa42a1d1fd55d.png"">"
local encoding typical problem	Sparseness resulting in:<br />- Curse of dimensionality<div>- Lack of generalization</div>
Local vs Distributed Encoding	Distributed encoding representation based on a combination of specific features of the input<br />Local encoding: a specific encoding for each element of the domain
Structuralist Theory	Concept of relationships to other concepts<div><br /></div><div>While Feature theory understands concepts as a combination of features.</div><div>NNs learn to represent structure through features</div>
Localist Representation	one neuron to each entity/concept/feature
Binding Problem	Combining multiple distributed representations means we lose information because we can't know which attributes belong to which entity
Jordan Network	A type of recurrent neural network where the output is fed back to the input (without internal reccurent connections)
SRN	Simple Recurrent Network (aka Elman network)<div><br></div><div>Copys hidden layer to context unit, meaning there is a fixed weight of one</div><div>This copy is used as additional&nbsp;<b>input</b> for the next recurrent step (meaning not anywhere in the net, just at the input layer)</div>
Back Propagation Through Time	- Unroll network through time<div>- calculate gradients</div><div>- update starting weight matrix using sum of gradients</div><div><br></div><div>Important: We calculate using the same weight matrix, there is no learning during processing of a sequence</div>
Initialization for RNN - Why is it special, how to do it?	Since output and/or hidden state is used as input and not yet computed when starting, we need to initialize this just like the weights<br><br>-&gt; initialize them randomly and then learn the values using the error gradient
Softmax formula	"<img class=latex src=""latex-c6d4c0964c395ee523ab7fcf9f2d04eba9833dd1.png"">"
CTC	<div>- Connectionist Temporal Classification</div>- label sequences are considered equal if they differ only in alignment<div>- specifiation for nn output and a loss function that takes this output</div><div>- for temporal classification</div>
How is word2vec trained?	- CBOW: given previous and following words, predict word in the middle<div>- skip-gram: given a word, predict previous and following words</div>
CBOW vs skip-gram	CBOW is faster to train, slightly better accuracy for frequent words<div>skip-gram good for little data, better for rare words</div>
Components of Recursive Autoassociative Memory	- Compressor (encoder)<div>- Reconstructor (decoder)</div>
Advantage of RAAM	Use internal representation with enclosing input to represent recursive structures<div>Thereby its able to represent nested structures like binary trees Parent(X, Parent(Y, Z))</div>
t-SNE	Stochastic Neighbour Embeddings<div>technique for dimensionality reduction</div><div>good for visualization</div>
Echo State Networks	- train only the output layer<div>- use giant fully connected layer with random weights</div><div>- linear methods for training (e.g. linear regression)</div>
Reservoir state vector update using leaky integrator neurons	"<img class=latex src=""latex-34ab75b03717e5d49d52b19945f23bbded865062.png""></div></div><div><br></div><div>- <img class=latex src=""latex-c2d27638f6b128907359a66540f4105e088df896.png"">: is the state vector</div><div>- <img class=latex src=""latex-65934e62c7adf123e9849a7b9a163958162c1a74.png"">: input vector</div><div>- <img class=latex src=""latex-10cb764f88509fb1c8012366993fdbee98f31bc5.png"">: reservoir weight matrix</div><div>- <img class=latex src=""latex-81ec7b7290fcd56d6a98b765f2cba252c6b49903.png"">: input weight matrix</div><div>-&nbsp;<img class=latex src=""latex-1dc1c0119a604b91be9142370dc3159b6a9bbcb9.png""> time factor: the higher, the less the reservoir forgets</div><div>-&nbsp;<img class=latex src=""latex-bb2c93730dbb48558bb3c4738c956c4e8f816437.png""> is the activation function (e.g. tanh)</div>"
What are the components of an ESN?	"-&nbsp;<img class=latex src=""latex-d453a20c54c3d06647ffaca26ca3966a00bf10d2.png"">: input, reservoir and output layer weights<div>- only&nbsp;<img class=latex src=""latex-16efc0b94d6696a7f7c7c8c2cf868b2b05201d93.png""> is trained, the rest is fixed random</div><div>-&nbsp;<img class=latex src=""latex-4485828f5a19c01ef573976d83d057fa840ed1e3.png""> state vector at time&nbsp;<img class=latex src=""latex-e0d2bf360290fd61d1c1557e763f2622363b3d35.png""></div>"
Offline learning for ESNs	"<img class=latex src=""latex-8c4e3d76c5e89bc92b34faca560346d7510e5641.png""><div><br></div><div>- Y: all target values</div><div>- Z: all reservoir states over time</div><div>- Where + is the pseudo inverse of a matrix</div>"
Intialization of ESN Weights	"- use a centered distribution, 5-30% non-zero connections<div>- use input scaling factor for input weights</div><div>- use spectral radius to control ""temperature"" of reservoir dynamics</div>"
Spectral Radius of matrix W	"Absolute of maximal eigenvalue of W<br>""scaling factor"" of the transformation described by W"
Echo State Property	When spectral radius &lt; 1, the system is able to forget its initial state, not amplifying any values
Recurrent Plausiblity Network	For every hidden layer, keep a Context copy:<div>context scaled by hysteresis factor + hidden layer scaled by (1 - hysteresis factor)</div>
Components of an LSTM	- forget gate<div>- input gate</div><div>- output gate</div><div>- cell state<br></div>
Activation process of an LSTM cell	"<img src=""LSTM3-chain.png"">"
How does a Multiple Timescale Recurrent Neural Network	- Multiple contexts in one layer: each neuron as a time scale<div>- time scale T controls how long previous states are retained for each neuron</div>
Clockwork RNN	Every neuron has a clock that determines update frequency<div>if neuron is inactive, returns activation of last time step</div>
pros/cons of clockwork RNNs	pro:<div>- capture different timescales in data</div><div>- capture temporal dependencies efficiently</div><div>- can remember long sequences</div><div>con:</div><div>- clock frequencies highly dependent on data</div><div>- can't learn clock frequencies</div>
Differentiable Neural Computer	- Attention mechanism allows differentiable memory access<div>- promises transfer learning</div><div>- still very new</div>
Auto Encoder	Special case of MLP<div>Train 'unsupervised' to reproduce the input as output after at least one hidden layer</div><div>Hidden layer is a representation of the input</div>
Transpose Weights Model	"Instead of bakpropagtion only train <img class=latex src=""latex-105b35d6b719e3b65c6bb9155ba7c07cc77aae84.png""> and set <img class=latex src=""latex-f1eb291b8fcc1d0c9943e54aa1e3f517f534005b.png""> to be <img class=latex src=""latex-1deea962c7e9319351310d330a487a1e220ce155.png"">"
Simple Auto Encoder Components	"- Three NN layers, input, hidden, output<div>- <img class=latex src=""latex-f1eb291b8fcc1d0c9943e54aa1e3f517f534005b.png""> weights to get from input to hidden layer</div><div>- <img class=latex src=""latex-105b35d6b719e3b65c6bb9155ba7c07cc77aae84.png""> weights to get from hidden to output layer</div><div>- Activation function for first weight matrix, error function for second one</div>"
Transfer Function	Different name for activation function
Helmholtz Machine	"Alternative to backpropagation or transposed weights<div>Uses wake-sleep-algorithm</div><div><br></div><div>Wake phase: normal training of decoder</div><div><br></div><div>Sleep phase:&nbsp;</div><div>- Generate random hidden activities&nbsp;<img class=latex src=""latex-bafe4ac4ce5ba0a021a9c4db70ad93892f63e7ee.png""></div><div>- Generate 'imagined' inputs from these&nbsp;<img class=latex src=""latex-6a1a23159b408ba61f88d2ea989fffa94779cf9b.png""></div><div>- Sleep training:&nbsp;<img class=latex src=""latex-c037a40edbaeae1a7a2e6340085ef0646c99c042.png""></div><div><br></div><div>Meaning the first layer is trained by knowing the hidden state a sample would be generated from</div>"
Why are constraints needed to produce generative models?	"Perfect reproduction of the input would be&nbsp;<img class=latex src=""latex-9f2df50bcbf98baffe3e1da75b4042f4b5c79d5f.png""> where I is the identity matrix<div><br></div><div>This doesn't extract meaningful features</div>"
Different Constraints on Generative Model	Structure:<div>- fewer hidden neurons<br><div>Code</div><div>- weight constraints (eg L1 or L2)</div><div>- sparse hidden activations</div></div><div>- non-negativity (non-negative weights)</div><div>- denosing (reconstruct from partially corrupted input)</div>
Retinal Preprocessing	"Filter on spatial fequency:<br><img class=latex src=""latex-36032fa3a41ef8cffee9d40f19e562656fb291e3.png""><div>Where <img class=latex src=""latex-bb2c93730dbb48558bb3c4738c956c4e8f816437.png""> itself is in charge of reducing low frequencies while the&nbsp;<img class=latex src=""latex-a3a59bb1293ee3f6dec19de4019a7178874219ae.png""> termn reduces high frequencies<br></div>"
Sparse Coding	"A specific neuron is only rarely active<div>its distribution of activity strenght is highly peaked around 0 with long tails (as opposed to the gaussian distribution as shown in the figure)</div><div><br></div><div><img src=""paste-01fb9de513dde4002412df715f0494e3bbcb9c5b.jpg""><br></div>"
Sparse Coding Activation Function	- Has to reduce small activations but retain large ones<div>- Additional weight decay is needed to retain sparseness during training</div>
CNN Layer Components	Normalization<div>Conv</div><div>Pooling</div>
Local Contrast Normalization	"Substract local mean, devide by local standard deviation<div><br></div><div><img class=latex src=""latex-1dad5b186c3405a60b2a6f71863dd147d8e1bed3.png""><br></div><div><br>Where N is the neighborhood of a given pixel</div>"
Convolution Layer	A weighted moving sum over the input (often an image)<div>- Serveral filters per layer (feature redundancy for different features)<br><div><div>- Weights are shared for the whole image</div></div></div>
Pooling Layer	- Reduce dimensionality by subsampling over window<br>- Usually max or average
Training CNNs	- Gradients using transpose of the filter<div>- Tend to overfit -&gt; we need regularization</div>
Batch Normailization	Normalize activation of the previous layer at each epoch (such that mean is ~0 and standard deviation ~1)
Dropout	Prevent overfitting by temporarily removing units from the network, yields a more robust model<div><br></div><div>Could be interpreted as building an ensamble of different models</div><div><br></div><div>rescale outputs in testing (to have the same scale with additional units)</div>
Filter visualizations	Send random image, backpropagate gradient of a specific filters activations<div>Use those to update the random image</div>
Attention Maps	Visualzie gradient of a specific filters activation for a given image
Adversarial Learning	Generator and Discriminator<div>- Generator generates images from noise</div><div>- Discriminator decides wheter image is real<br><br></div><div>Error metric is output of the discriminator minus the actual label</div><div><br></div><div>Unsupervised architecture, only requires a set of real images to learn how to genereate new ones and discern real ones from generated ones</div>
3 possibilities of when to update weights	&nbsp;SGD: update after every sample<div>batch gradient descent: after seeing all samples</div><div>mini-batch gradient descent: after seeing some samples</div>
Why does computational model of computational graphs fit neural networks?	<div>Efficient gradient computation<br>Easy parallelization</div>
Reverse Mode Differentation	Use computational graph to factor along their paths, thereby getting derivatios of the error with respect to every node/layer
3 different optimizers	Adagrad<div>RMSProp</div><div>Adam</div>
Adagrad	Maintains per-parameter learning rate (sum of past gradients). Making it possible to increase the influence of rare but informative features
RMSProp	Like Adagrad but scales learning rate by expoentially decacying mean of gradients (meaning an average thats biased towards the last values)
Adam	Looks at mean and variance of past gradients to determine a per parameter learning rate
Covariate Shift	Change in the distribution of the independent variables (covariates)
Overfitting typically occurs when	Training for too long<div>Too few training samples</div><div>too many parameters</div>
Solutions to overfitting	More training data<div>Reduce model compexity</div><div>cross validation, early stopping</div><div>Regularaization</div>
Cross Validation	Split into three sets<div><br></div><div>Training, Validation, Test</div>
Regularization Formula	"<img class=latex src=""latex-98e69994bd777d2026e2453eed6df82abc18d3fb.png""><div>Where L is the loss function</div><div><img class=latex src=""latex-ce4588fd900d02afcbd260bc07f54cce49a7dc4a.png""> controls the impact of the regularization term<br></div>"
Action Value Function	"<img class=latex src=""latex-a30e9272bf730f9c2cfc13fcc1beb3a933eff1cb.png""> the value of a given action&nbsp;<img class=latex src=""latex-c7d457e388298246adb06c587bccd419ea67f7e8.png""> while in the state&nbsp;<img class=latex src=""latex-f37bba504894945c07a32f5496d74299a37aa51c.png"">"
Temporal Difference Learning	"Problem: Reward sparse environment, meaning we don't get a reward right away<div><br></div><div>Solution: <img class=latex src=""latex-2d12b9ae6cee40ee0e6c90f3f40eee5d85f08d49.png""><br>We add a discounted reward for the next action to allow for long term plannning&nbsp;<img class=latex src=""latex-66981fa3920210c6ad8dbe5e968783d5dd7520c3.png""> controls the far-sigtedness of the agent</div><div>large <img class=latex src=""latex-66981fa3920210c6ad8dbe5e968783d5dd7520c3.png""> values mean long term planning</div>"
Greedy Reinforcement Learning Strategy	"<img class=latex src=""latex-b36d0fa5bbe821876a9c6353fa17b7486af37a17.png""> always take the actikon with the highest reward<br><div><br></div><div>BUT: no exploration</div>"
"<img class=latex src=""latex-eaf4418fbe935c15a606516d8f55dc380cd8e822.png"">-Greedy Action Policy"	"Select a random action with probability <img class=latex src=""latex-eaf4418fbe935c15a606516d8f55dc380cd8e822.png"">"
Q-Learning	"Update based on next best possible estimates<div>Off-policy, we ignore our policy (e.g. <img class=latex src=""latex-eaf4418fbe935c15a606516d8f55dc380cd8e822.png"">-greedy) and just take the action with the highest value<div><div><img class=latex src=""latex-176ec6afb062d74ba453acaa96a5c8b501933477.png""><br></div></div></div>"
SARSA	"On-policy, we update on the actual action that would be chosen next<div><img class=latex src=""latex-726f7db9c29609e58c85f88cb7f99e185b8e9b3d.png""><br></div>"
Actor Critic Update Formular (state value)	"<img class=latex src=""latex-aae85f0663e291b48a3af0e272178eed859fe3be.png""><br />updated by difference from critic"
Stability Problem in Reinforcment Learning	Reinforcment learning tends to be unstable, meaning different random initializations get wildly different results unlesss some measures are taken.<div>This happens due to temporally correlated input data.<br><div><br></div><div>e.g.&nbsp;</div><div>- Replay Memory</div><div>- Double Q-Learning</div><div>- Q-Freezing</div><div>- Only gradual updating active copy from a trained copy</div></div>
Challenge Solved By Actor-Critic Model	Continious Action Space
Describe the Actor-Critic System and how it's trained	- Two networks: Actor and Critic<div>- Actor learns policy given a state</div><div>- Critic learns Q(s,a) given s and a</div><div>- Actor cannot be trained directly</div><div>- Critic is trained as normal</div><div>- Gradients of critic with regards to action are used to update actor</div>
Q-Freezing	Keep constant copy of Q-network, only update after a time interval (this prevents unstableness, avoiding local minima)
Double Q learning	Use 2 Q-Functions, with one updating the other<div>This is an attempt to fix the stability issue</div>
Boltztmann Action Policy	"<img class=latex src=""latex-06f8a1dec6495d449ca6347723975e184c1ea57d.png""><div>Softmax operation</div><div>Large&nbsp;<img class=latex src=""latex-1dc1c0119a604b91be9142370dc3159b6a9bbcb9.png""> leads to exploration (exponent of e gets smaller, equalizing all probabilities to a larger extent)</div>"
Self-Organzing Map Algorithm	"for each input<div>&nbsp; &nbsp;find closest neuron</div><div>&nbsp; &nbsp;update that neuron (and its neighbors to be closer to input)</div><div><br></div><div>update function:<br><br></div><div><img class=latex src=""latex-fc0a4a3e5005ab17ab62715f40e20c4a43c5c6cc.png""><br></div>"
Neighborhood Function SOM	"Add the term&nbsp;<img class=latex src=""latex-2ed06117d8702156ae3003d1650d78c46530d471.png""> to the calculation where&nbsp;<img class=latex src=""latex-470e31942b5204d0b59d83c3266c5fcb5e230fb7.png""> is the distance of the best matching unit to the one being updated.<div>This results in the overall update rule</div><div><img class=latex src=""latex-64a0b7236ac8439169cb4d650ef46584b60e95c9.png""><br></div><div><br></div><div>The functionion usually is a gaussian, meaning a quick falloff with a long tail<br>The width of the gaussian curve decreases over time</div>"
U-Matrix	Interpret distances of neurons (which are cluster centers), take the average distance of a neuron to its neightbors<div><br></div><div>Short for: unified distance matrix</div>
Hindsight Experience Replay	- Set a goal<div>- Peform actions</div><div>- if goal was not met act as though this is what you meant to do and learn from this</div>
Ridge Regression ESN	"Solves equation using&nbsp;Tikhonov regularization (a form of ridge regression)<br><img class=latex src=""latex-af067158076c5821f54109f3ae32d52362fc2d5d.png""><div><br></div><div>Punishes large weights as well as deviations from training set meaning a solution with smaller weights is preffered. Large&nbsp;<img class=latex src=""latex-fdb63b9e51abe6bbb16acfb5d7b773ddbb5bf4a8.png""> values make small weights more important</div>"
Growing When Required	Algorithm for growing self organizing maps<div><br></div><div>Add neurons when the current best matching neuron is insufficiently activated and has been activated enough in the past (to prevent untrained neurons leading to many more being created)</div>
Growing neural gas	Self organizing maps growing after a set number of steps<div>Add them wherever the acumulated error was highest&nbsp;</div>
Perceptron Activation Function	"<img class=latex src=""latex-c518ce632d474f407b66e4ec8afc69097c52db76.png"">"
"Delta rule: why is the input <img class=latex src=""latex-67bc6daa9d6b964201d6cef60cbeb1ac5fd26ead.png"">&nbsp;a factor?"	"<img class=latex src=""latex-26dc3f735a3565927f88d58ecad3d308216faa8e.png""><div><img class=latex src=""latex-67bc6daa9d6b964201d6cef60cbeb1ac5fd26ead.png""> adjusts the update proportionally to the input - If we left this out, every weight would get the same update. But weights have different influence on the output depending on their input.<br></div>Fehler beim Ausf√ºhren von latex.<br>Erzeugte Datei: /tmp/anki_temp/tmp.tex<br>Haben Sie LaTeX und dvipng/dvisvgm installiert?"
Backpropagation update rule with weight decay	"<img class=latex src=""latex-b01edfb913bd3681443ff403e4cfb46ef950db38.png"">"
What does weight decay do? What are its advantages?	- Continuously shrinks weights<div>- helps avoid overfitting</div>
Todo update formula for Jordan network	
update formula for srn	after unfolding the network over time the same as a normal feedforward network
update formula for mtrnn	
update formula for rpn	"<img class=latex src=""latex-d5d44b88a241b2d0c34fd8f353b4dcaeb43b05f1.png""><div><br><div>Where&nbsp;</div><div><br></div><div><img class=latex src=""latex-7cfbd293bd5e79907677455dc6293173a6c8caee.png""></div><div><br></div><div>and</div><div><br></div><div><img class=latex src=""latex-fabccda7ff81e952806308354edc6aa4133d2dfa.png""></div><div><br></div><div><img class=latex src=""latex-8e0b9f92fa1d1f3b8b4c42698703c470dad70615.png""> is the maximal time step, <img class=latex src=""latex-c3355896da590fc491a10150a50416687626d7cc.png""> is the context layer,&nbsp;<img class=latex src=""latex-027f4a11d6090f9eac0ce2488df6384dad1263ea.png""> the input layer<br></div></div>"
Activation formula for RPN's context layer	"<img class=latex src=""latex-5073b21dd37036122a4dbba635222184b26962ff.png""><div><br></div><div>for neuron&nbsp;<img class=latex src=""latex-34857b3ba74ce5cd8607f3ebd23e9015908ada71.png""> in context layer&nbsp;<img class=latex src=""latex-174fadd07fd54c9afe288e96558c92e0c1da733a.png""></div>"
